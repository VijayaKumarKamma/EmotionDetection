{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.00003000003\n",
      "Inside if \n",
      "sunglasses (19.45%)\n",
      "Inside if \n",
      "sunglasses (15.95%)\n",
      "Inside if \n",
      "sunglasses (14.11%)\n",
      "Inside if \n",
      "sunglasses (16.09%)\n",
      "Inside if \n",
      "sunglasses (16.15%)\n",
      "Inside if \n",
      "sunglasses (18.89%)\n",
      "Inside if \n",
      "sunglasses (13.33%)\n",
      "Inside if \n",
      "sunglasses (17.49%)\n",
      "Inside if \n",
      "sunglasses (20.17%)\n",
      "Inside if \n",
      "sunglasses (17.06%)\n",
      "Inside if \n",
      "sunglasses (18.21%)\n",
      "Inside if \n",
      "sunglasses (19.06%)\n",
      "Inside if \n",
      "sunglasses (13.97%)\n",
      "Inside if \n",
      "lab_coat (11.69%)\n",
      "Inside if \n",
      "sunglasses (11.62%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#face_cascade = cv2.CascadeClassifier(\"C:\\\\Users\\\\kamma\\\\Anaconda3\\\\envs\\\\vdpnLeP35\\\\Library\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_default.xml\")\n",
    "#face_cascade = cv2.CascadeClassifier(\"C:\\\\Users\\\\kamma\\\\Anaconda3\\\\envs\\\\vdpnLeP35\\\\Library\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_alt_tree.xml\")\n",
    "face_cascade = cv2.CascadeClassifier(\"C:\\\\Users\\Allworx User 2.ALLWORXW2\\\\Anaconda3\\\\envs\\\\tf15\\\\Library\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_default.xml\")\n",
    "#face_cascade = cv2.CascadeClassifier(\"C:\\\\Users\\\\kamma\\\\Anaconda3\\\\envs\\\\vdpnLeP35\\\\Library\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_alt2.xml\")\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load the model\n",
    "model = VGG16()\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(5, 30)\n",
    "frameRate = cap.get(5)\n",
    "print(frameRate)\n",
    "count=0\n",
    "while True:\n",
    "    #current frame number--this is for frame rate\n",
    "    count+=1\n",
    "    frameId=count\n",
    "    ret, img = cap.read()\n",
    "    #frameId = cap.get(1)\n",
    "    #show the mark on face\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    #print(frameId)\n",
    "    #print(frameRate)\n",
    "    if (frameId % (math.floor((frameRate/5))) == 0):\n",
    "        print(\"Inside if \") \n",
    "        #converting ndarray to image\n",
    "        filename =\"vijay.jpg\"\n",
    "        cv2.imwrite(filename, img) \n",
    "\n",
    "        # load an image from file\n",
    "        image = load_img('vijay.jpg', target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # predict the probability across all output classes\n",
    "        yhat = model.predict(image)\n",
    "        # convert the probabilities to class labels\n",
    "        label = decode_predictions(yhat)\n",
    "        # retrieve the most likely result, e.g. highest probability\n",
    "        label = label[0][0]\n",
    "        # print the classification\n",
    "        print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "        #Write text on the rectanguler box.\n",
    "        cv2.putText(img, str(label), (286, 104), font, 0.8, (0, 0, 255), 3, cv2.LINE_AA)     \n",
    "        cv2.imshow('img',img) \n",
    "    k= cv2.waitKey(5) & 0xff\n",
    "    if k==27:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n",
    "\n",
    "\n",
    "#https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_table_of_contents_gui/py_table_of_contents_gui.html#py-table-of-content-gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "SSDBoxSizes = collections.namedtuple('SSDBoxSizes', ['min', 'max'])\n",
    "\n",
    "Spec = collections.namedtuple('Spec', ['feature_map_size', 'shrinkage', 'box_sizes', 'aspect_ratios'])\n",
    "\n",
    "# the SSD orignal specs\n",
    "specs = [\n",
    "    Spec(38, 8, SSDBoxSizes(30, 60), [2]),\n",
    "    Spec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n",
    "    Spec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n",
    "    Spec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n",
    "    Spec(3, 100, SSDBoxSizes(213, 264), [2]),\n",
    "    Spec(1, 300, SSDBoxSizes(264, 315), [2])\n",
    "]\n",
    "\n",
    "def generate_ssd_priors(specs, image_size=300, clip=True):\n",
    "    \"\"\"Generate SSD Prior Boxes.\n",
    "    \n",
    "    Args:\n",
    "        specs: Specs about the shapes of sizes of prior boxes. i.e.\n",
    "            specs = [\n",
    "                Spec(38, 8, SSDBoxSizes(30, 60), [2]),\n",
    "                Spec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n",
    "                Spec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n",
    "                Spec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n",
    "                Spec(3, 100, SSDBoxSizes(213, 264), [2]),\n",
    "                Spec(1, 300, SSDBoxSizes(264, 315), [2])\n",
    "            ]\n",
    "        image_size: image size.\n",
    "    \n",
    "    Returns:\n",
    "        priors: a list of priors: [[center_x, center_y, h, w]]. All the values\n",
    "            are relative to the image size (300x300).\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    for spec in specs:\n",
    "        scale = image_size / spec.shrinkage\n",
    "        for j, i in itertools.product(range(spec.feature_map_size), repeat=2):\n",
    "            x_center = (i + 0.5) / scale\n",
    "            y_center = (j + 0.5) / scale\n",
    "\n",
    "            # small sized square box\n",
    "            size = spec.box_sizes.min\n",
    "            h = w = size / image_size\n",
    "            boxes.append([\n",
    "                x_center,\n",
    "                y_center,\n",
    "                h,\n",
    "                w\n",
    "            ])\n",
    "            \n",
    "            # big sized square box\n",
    "            size = np.sqrt(spec.box_sizes.max * spec.box_sizes.min)\n",
    "            h = w = size / image_size\n",
    "            boxes.append([\n",
    "                x_center,\n",
    "                y_center,\n",
    "                h,\n",
    "                w\n",
    "            ])           \n",
    "            \n",
    "            # change h/w ratio of the small sized box\n",
    "            # based on the SSD implementation, it only applies ratio to the smallest size.\n",
    "            # it looks wierd.\n",
    "            size = spec.box_sizes.min\n",
    "            h = w = size / image_size\n",
    "            for ratio in spec.aspect_ratios:\n",
    "                ratio = sqrt(ratio)                  \n",
    "                boxes.append([\n",
    "                    x_center,\n",
    "                    y_center,\n",
    "                    h * ratio,\n",
    "                    w / ratio\n",
    "                ])\n",
    "                boxes.append([\n",
    "                    x_center,\n",
    "                    y_center,\n",
    "                    h / ratio,\n",
    "                    w * ratio\n",
    "                ])\n",
    "            \n",
    "\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    if clip:\n",
    "        boxes = np.clip(boxes, 0.0, 1.0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
